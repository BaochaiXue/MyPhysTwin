from __future__ import annotations

"""Sample training data from filtered trajectories and optional shape priors."""

from typing import Dict, MutableMapping, Sequence

import numpy as np  # Numerical backbone for point manipulations.
import open3d as o3d  # Provides point-cloud operations and rendering utilities.
import pickle  # Serialises processed data back to disk.
import matplotlib.pyplot as plt  # Supplies colour maps for visualising trajectories.
import trimesh  # Loads and samples meshes produced by TRELLIS.
import cv2  # Writes diagnostic videos showcasing the final data.
from utils.align_util import as_mesh  # Converts heterogeneous trimesh objects into a canonical mesh.
from argparse import ArgumentParser  # Parses CLI arguments selecting dataset and options.

parser = ArgumentParser()
parser.add_argument(
    "--base_path",
    type=str,
    required=True,
)
parser.add_argument("--case_name", type=str, required=True)
parser.add_argument("--shape_prior", action="store_true", default=False)
parser.add_argument("--num_surface_points", type=int, default=1024)
parser.add_argument("--volume_sample_size", type=float, default=0.005)
args = parser.parse_args()

base_path = args.base_path
case_name = args.case_name

# Used to judge if using the shape prior
SHAPE_PRIOR = args.shape_prior
num_surface_points = args.num_surface_points
volume_sample_size = args.volume_sample_size


def getSphereMesh(
    center: Sequence[float],
    radius: float = 0.1,
    color: Sequence[float] = (0, 0, 0),
) -> o3d.geometry.TriangleMesh:
    """Create an Open3D sphere used to mark controller anchor points in visualisations.

    Args:
        center (Iterable[float]): XYZ coordinates of the sphere centre.
        radius (float, optional): Radius of the sphere marker. Defaults to 0.1.
        color (Iterable[float], optional): RGB colour for the marker surface. Defaults to black.

    Returns:
        o3d.geometry.TriangleMesh: Sphere mesh translated to ``center``.
    """

    sphere = o3d.geometry.TriangleMesh.create_sphere(radius=radius).translate(center)
    sphere.paint_uniform_color(color)
    return sphere


def process_unique_points(
    track_data: MutableMapping[str, np.ndarray]
) -> MutableMapping[str, np.ndarray]:
    """Remove duplicate object tracks, optionally blend in a shape prior, and export diagnostics.

    Args:
        track_data (Dict[str, np.ndarray]): Filtered trajectories generated by ``data_process_track``.

    Returns:
        Dict[str, np.ndarray]: Updated dictionary with deduplicated object points and optional prior samples.
    """

    object_points = track_data["object_points"]
    object_colors = track_data["object_colors"]
    object_visibilities = track_data["object_visibilities"]
    object_motions_valid = track_data["object_motions_valid"]
    controller_points = track_data["controller_points"]

    # Compute a unique index set so duplicated pixels (common across camera overlap) are removed.
    first_object_points = object_points[0]
    unique_idx = np.unique(first_object_points, axis=0, return_index=True)[1]
    object_points = object_points[:, unique_idx, :]
    object_colors = object_colors[:, unique_idx, :]
    object_visibilities = object_visibilities[:, unique_idx]
    object_motions_valid = object_motions_valid[:, unique_idx]

    # Clamp any point predicted below the table plane back onto z = 0 to avoid thin tails in the sample grid.
    object_points[object_points[..., 2] > 0, 2] = 0

    if SHAPE_PRIOR:
        shape_mesh_path = f"{base_path}/{case_name}/shape/matching/final_mesh.glb"
        trimesh_mesh = trimesh.load(shape_mesh_path, force="mesh")
        trimesh_mesh = as_mesh(trimesh_mesh)
        # Sample both surface and interior points from the aligned mesh to densify the object volume.
        surface_points, _ = trimesh.sample.sample_surface(
            trimesh_mesh, num_surface_points
        )
        interior_points = trimesh.sample.volume_mesh(trimesh_mesh, 10000)

    # Build a voxel grid keyed by ``volume_sample_size`` to prioritise object points and avoid redundant samples.
    if SHAPE_PRIOR:
        all_points = np.concatenate(
            [surface_points, interior_points, object_points[0]], axis=0
        )
    else:
        all_points = object_points[0]
    min_bound = np.min(all_points, axis=0)
    index = []  # Indices of object points that occupy new voxels.
    grid_flag = {}  # Tracks which voxels have already been filled.
    for i in range(object_points.shape[1]):
        grid_index = tuple(
            np.floor((object_points[0, i] - min_bound) / volume_sample_size).astype(int)
        )
        if grid_index not in grid_flag:
            grid_flag[grid_index] = 1
            index.append(i)

    if SHAPE_PRIOR:
        final_surface_points = []
        for i in range(surface_points.shape[0]):
            grid_index = tuple(
                np.floor((surface_points[i] - min_bound) / volume_sample_size).astype(
                    int
                )
            )
            if grid_index not in grid_flag:
                grid_flag[grid_index] = 1
                final_surface_points.append(surface_points[i])
        final_interior_points = []
        for i in range(interior_points.shape[0]):
            grid_index = tuple(
                np.floor((interior_points[i] - min_bound) / volume_sample_size).astype(
                    int
                )
            )
            if grid_index not in grid_flag:
                grid_flag[grid_index] = 1
                final_interior_points.append(interior_points[i])
        all_points = np.concatenate(
            [final_surface_points, final_interior_points, object_points[0][index]],
            axis=0,
        )
    else:
        all_points = object_points[0][index]

    # Render a turntable video of the densified point cloud so users can validate sampling quality.
    all_pcd = o3d.geometry.PointCloud()
    all_pcd.points = o3d.utility.Vector3dVector(all_points)
    coorindate = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.1)

    vis = o3d.visualization.Visualizer()
    vis.create_window(visible=False)
    dummy_frame = np.asarray(vis.capture_screen_float_buffer(do_render=True))
    height, width, _ = dummy_frame.shape
    fourcc = cv2.VideoWriter_fourcc(*"avc1")
    video_writer = cv2.VideoWriter(
        f"{base_path}/{case_name}/final_pcd.mp4", fourcc, 30, (width, height)
    )

    vis.add_geometry(all_pcd)
    # vis.add_geometry(coorindate)
    view_control = vis.get_view_control()
    for j in range(360):
        view_control.rotate(10, 0)
        vis.poll_events()
        vis.update_renderer()
        frame = np.asarray(vis.capture_screen_float_buffer(do_render=True))
        frame = (frame * 255).astype(np.uint8)
        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
        video_writer.write(frame)
    vis.destroy_window()

    # Replace the object entries with the deduplicated subset retained by ``index``.
    track_data.pop("object_points")
    track_data.pop("object_colors")
    track_data.pop("object_visibilities")
    track_data.pop("object_motions_valid")
    track_data["object_points"] = object_points[:, index, :]
    track_data["object_colors"] = object_colors[:, index, :]
    track_data["object_visibilities"] = object_visibilities[:, index]
    track_data["object_motions_valid"] = object_motions_valid[:, index]
    if SHAPE_PRIOR:
        track_data["surface_points"] = np.array(final_surface_points)
        track_data["interior_points"] = np.array(final_interior_points)
    else:
        track_data["surface_points"] = np.zeros((0, 3))
        track_data["interior_points"] = np.zeros((0, 3))

    return track_data


def visualize_track(track_data: Dict[str, np.ndarray]) -> None:
    """Render an animated preview of the filtered object and controller trajectories.

    Args:
        track_data (Dict[str, np.ndarray]): Dictionary produced by :func:`process_unique_points`.

    Returns:
        None
    """

    object_points = track_data["object_points"]
    object_colors = track_data["object_colors"]
    object_visibilities = track_data["object_visibilities"]
    object_motions_valid = track_data["object_motions_valid"]
    controller_points = track_data["controller_points"]

    frame_num = object_points.shape[0]

    vis = o3d.visualization.Visualizer()
    vis.create_window(visible=False)
    dummy_frame = np.asarray(vis.capture_screen_float_buffer(do_render=True))
    height, width, _ = dummy_frame.shape
    fourcc = cv2.VideoWriter_fourcc(*"avc1")
    video_writer = cv2.VideoWriter(
        f"{base_path}/{case_name}/final_data.mp4", fourcc, 30, (width, height)
    )

    controller_meshes = []
    prev_center = []

    y_min, y_max = np.min(object_points[0, :, 1]), np.max(object_points[0, :, 1])
    y_normalized = (object_points[0, :, 1] - y_min) / (y_max - y_min)
    rainbow_colors = plt.cm.rainbow(y_normalized)[:, :3]

    for i in range(frame_num):
        object_pcd = o3d.geometry.PointCloud()
        object_pcd.points = o3d.utility.Vector3dVector(
            object_points[i, np.where(object_visibilities[i])[0], :]
        )
        object_pcd.colors = o3d.utility.Vector3dVector(
            rainbow_colors[np.where(object_visibilities[i])[0]]
        )  # Colour encode points by vertical position for easier reading.

        if i == 0:
            render_object_pcd = object_pcd
            vis.add_geometry(render_object_pcd)
            # Use sphere meshes to visualise each controller anchor trajectory.
            for j in range(controller_points.shape[1]):
                origin = controller_points[i, j]
                origin_color = [1, 0, 0]
                controller_meshes.append(
                    getSphereMesh(origin, color=origin_color, radius=0.01)
                )
                vis.add_geometry(controller_meshes[-1])
                prev_center.append(origin)
            # Adjust the viewpoint to a canonical angle for all turntable renders.
            view_control = vis.get_view_control()
            view_control.set_front([1, 0, -2])
            view_control.set_up([0, 0, -1])
            view_control.set_zoom(1)
        else:
            render_object_pcd.points = o3d.utility.Vector3dVector(object_pcd.points)
            render_object_pcd.colors = o3d.utility.Vector3dVector(object_pcd.colors)
            vis.update_geometry(render_object_pcd)
            for j in range(controller_points.shape[1]):
                origin = controller_points[i, j]
                controller_meshes[j].translate(origin - prev_center[j])
                vis.update_geometry(controller_meshes[j])
                prev_center[j] = origin
            vis.poll_events()
            vis.update_renderer()

        frame = np.asarray(vis.capture_screen_float_buffer(do_render=True))
        frame = (frame * 255).astype(np.uint8)
        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
        video_writer.write(frame)


if __name__ == "__main__":
    with open(f"{base_path}/{case_name}/track_process_data.pkl", "rb") as f:
        track_data = pickle.load(f)  # Load motion-filtered trajectories produced by the tracking pipeline.

    track_data = process_unique_points(track_data)  # Deduplicate, voxel-sample, and integrate optional shape prior points.

    with open(f"{base_path}/{case_name}/final_data.pkl", "wb") as f:
        pickle.dump(track_data, f)  # Persist the final dataset consumed by training/inference scripts.

    visualize_track(track_data)  # Export a diagnostic video that visualises both object and controller trajectories.
